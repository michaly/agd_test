{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Printing config:\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "pd.set_option('display.expand_frame_repr', False) # for printing full objects\n",
    "\n",
    "# Running config:\n",
    "input_file_path = './mappinghotelsdataset.xlsx'\n",
    "\n",
    "p1_sheet_name = 'Partner1'\n",
    "p2_sheet_name = 'Partner2'\n",
    "example_sheet_name = 'examples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # HELPER FUNCTIONS # # # # # # \n",
    "\n",
    "def colNameListByDType(df, numericCols=True):\n",
    "    # # # # # # # # # #\n",
    "    # Finds the names of numeric/non-numeric columns of a dataframe\n",
    "    # Args:\n",
    "    #       df - (pandas dataframe)\n",
    "    #       numericCols - (bool), True - for numerical columns, False - for non-numerical columns\n",
    "    # Return:\n",
    "    #       col_name_list - (list of strings), the matched columns name\n",
    "    # # # # # # # # # #\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "    col_name_list = list()\n",
    "    for col in df.columns:\n",
    "        if(numericCols): # if the numeric columns are required\n",
    "            if(is_numeric_dtype(df[col]) == True):\n",
    "                col_name_list += [col]\n",
    "        else:   # the non-numeric columns are required\n",
    "            if (is_numeric_dtype(df[col]) == False):\n",
    "                col_name_list += [col]\n",
    "\n",
    "    # apply doesn't work with is_numeric_dtype for some reason!\n",
    "    #if(numericCols):\n",
    "    #    col_name_list = df.columns[df.apply(lambda x: is_numeric_dtype(x))]\n",
    "    #else:\n",
    "    #    col_name_list = df.columns[~np.array(df.apply(is_numeric_dtype))]\n",
    "\n",
    "    return col_name_list\n",
    "\n",
    "def getDfSliceRowIdx(df, colName, val):\n",
    "    # # # # # # # # # #\n",
    "    # Finds the row indices where colName == val in a dataframe\n",
    "    # Args:\n",
    "    #       df - (pandas dataframe)\n",
    "    #       colName - (string)\n",
    "    #       val - (string / int)\n",
    "    # Return:\n",
    "    #       (indices list), the row indices\n",
    "    # # # # # # # # # #\n",
    "    return df.loc[df[colName] == val].index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # DADA ANALYSIS # # # # # # \n",
    "\n",
    "def dataAnalysis(df):\n",
    "    # # # # # # # # # #\n",
    "    # Analyze dataframe\n",
    "    # Args:\n",
    "    #       df - (pandas datafrae)\n",
    "    # Return:\n",
    "    #       Nothing\n",
    "    # # # # # # # # # #\n",
    "    ignore_col_set = set(['p1.hotel_address', 'p2.hotel_address',\n",
    "                       'p1.hotel_name', 'p2.hotel_name',\n",
    "                       'p1.key', 'p2.key'])\n",
    "    print(\"\\nData shape : \" + str(df.shape))  \n",
    "    # Checking cols data type and existence of missing values \n",
    "    print((\"\\nData info: \\n%s\") % df.info())  \n",
    "    print((\"\\nDMissing values: \\n%s\") % df.isnull().sum( )) \n",
    "    # Basic statistics:\n",
    "    print((\"\\nSummary of numeric features: \\n%s\") % df.describe(include=[np.number]))  \n",
    "    print((\"\\nSummary of non-numeric features: \\n%s\") % df.describe(include=['O'])) \n",
    "\n",
    "    non_numeric_cols = list(set(colNameListByDType(df, numericCols=False)) - ignore_col_set)\n",
    "    for col in non_numeric_cols:\n",
    "        print((\"\\nTop unique value (normed) count of column : %s\\n%s\") % \n",
    "                (col, df[col].value_counts(normalize=True)[:5]))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # HELPER FUNCTIONS for preprocessing # # # # # # \n",
    "\n",
    "def convertToAscii(df):\n",
    "    # # # # # # # # # #\n",
    "    # Convert all non-numeric columns of a dataframe to ASCII\n",
    "    # Args:\n",
    "    #       df - (pandas dataframe)\n",
    "    # Return:\n",
    "    #       df - (pandas dataframe) converted df\n",
    "    # # # # # # # # # #\n",
    "    import unidecode #conda install -c anaconda unidecode \n",
    "    \n",
    "    def decode(x):\n",
    "        if(isinstance(x, str)): \n",
    "            x = unidecode.unidecode(x)\n",
    "        return x\n",
    "    \n",
    "    convert_cols = ['p1.city_name', 'p1.hotel_address', 'p1.hotel_name',\n",
    "                   'p2.city_name', 'p2.hotel_address', 'p2.hotel_name',]\n",
    "    \n",
    "    for col in (set(convert_cols) & set(df.columns)):\n",
    "        df.loc[:,col] = df.loc[:,col].apply(lambda x: decode(x))\n",
    "    return df\n",
    "\n",
    "def minMaxScaling(df, colsToScale):\n",
    "    # # # # # # # # # #\n",
    "    # Scale (MinMax) the specified columns of a dataset [0,1]\n",
    "    # Note: the scaling is applied to the original df, no copy is made!\n",
    "    # Args:\n",
    "    #       df - (pandas dataframe), the dataset\n",
    "    #       colsToScale - (list of strings), column names to scale.\n",
    "    # Return:\n",
    "    #       df - (pandas dataframe), the scaled dataset\n",
    "    # # # # # # # # # #\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    df.loc[:, colsToScale] = min_max_scaler.fit_transform(df.loc[:, colsToScale])\n",
    "    return df\n",
    "\n",
    "def removePattern(sr, pattern=\"-\"):\n",
    "    # # # # # # # # # #\n",
    "    # Remove pattern occurrances from series\n",
    "    # Args:\n",
    "    #       sr - (pandas series)\n",
    "    #       pattern - (string)\n",
    "    # Return:\n",
    "    #       (pandas series), the series without the pattern\n",
    "    # # # # # # # # # #\n",
    "    def removePtrn(x, pattern):\n",
    "        if(isinstance(x, str)): \n",
    "            x = x.replace(pattern, \"\")\n",
    "        return x\n",
    "    \n",
    "    return sr.apply(lambda x: removePtrn(x, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # DADA PREPROCESSING # # # # # # \n",
    "\n",
    "def preprocData(df):\n",
    "    # # # # # # # # # #\n",
    "    # Data preprocessing \n",
    "    # Args:\n",
    "    #       df - (pandas dataframe)\n",
    "    # Return:\n",
    "    #       df - (pandas dataframe), preprocessed df\n",
    "    # # # # # # # # # #\n",
    "    \n",
    "    # Convert all text columns to ASCII:\n",
    "    df = convertToAscii(df)\n",
    "    \n",
    "    # Handle missing values:\n",
    "    df.fillna('', inplace=True)\n",
    "    \n",
    "    # Scale star_rating to [0,1] scale:\n",
    "    star_rating_name_list = list(set(['p1.star_rating', 'p2.star_rating']) & set(df.columns))\n",
    "    df = minMaxScaling(df, star_rating_name_list)\n",
    "    \n",
    "    # Remove '-' occurrences in postal_code:\n",
    "    postal_code_name_list = list(set(['p1.postal_code', 'p2.postal_code']) & set(df.columns))\n",
    "    for i in postal_code_name_list:\n",
    "        df.loc[:, i] = removePattern(df.loc[:, i], pattern=\"-\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # HELPER FUNCTIONS for prediction # # # # # # \n",
    "\n",
    "def getTfidfFeatures(p1_sr, p2_sr, tf_threshold):\n",
    "    # # # # # # # # # #\n",
    "    # Generate tfidf features (fitted on both inputs)\n",
    "    # Args:\n",
    "    #       p1_sr - (pandas series of strings)\n",
    "    #       p2_sr - (pandas seriesof strings)\n",
    "    #       tf_threshold - (float [0,1]), represents the threshold for defining the stop words\n",
    "    # Return:\n",
    "    #       (p1_tfidf_csr, p2_tfidf_csr) - (tuple of numpy csr matrices), the tf-idf matrices\n",
    "    # # # # # # # # # #\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df=tf_threshold, binary=True, analyzer='word', encoding='utf-8') #, decode_error='ignore') #min_df=1, \n",
    "    \n",
    "    # Fit on both p1_sr and p2_sr:\n",
    "    vectorizer = vectorizer.fit(pd.concat([p1_sr, p2_sr])) # error on address - AttributeError: 'int' object has no attribute 'lower', but works on hotel_name that also has numbers!!\n",
    "    print((\"\\nVocabulary length: %d\") % len(vectorizer.vocabulary_))\n",
    "    print((\"\\nData-driven stop words - appeared in more than %d of the transactions:\\n%s\") % \n",
    "          (tf_threshold * (len(p1_sr)+len(p2_sr)), vectorizer.stop_words_))\n",
    "    \n",
    "    p1_tfidf_csr = vectorizer.transform(p1_sr) \n",
    "    p2_tfidf_csr = vectorizer.transform(p2_sr) \n",
    "\n",
    "    return (p1_tfidf_csr, p2_tfidf_csr)\n",
    "\n",
    "\n",
    "# # # # # # HELPER FUNCTIONS - distance measurements # # # # # # \n",
    "\n",
    "def calcCosineSim(vec_csr, mat_csr):\n",
    "    # # # # # # # # # #\n",
    "    # Compute cosine similarity between a vector and a (row-normalized) matrix (of tfidf)\n",
    "    # Args:\n",
    "    #       vec_csr - (numpy sparse vector)\n",
    "    #       mat_csr - (numpy sparse matrix)\n",
    "    # Return:\n",
    "    #       (list of floats), the dot product of the vector and the matrix\n",
    "    # # # # # # # # # #\n",
    "    from sklearn.metrics.pairwise import linear_kernel\n",
    "    return linear_kernel(vec_csr, mat_csr).flatten()\n",
    "\n",
    "def levenshteinDist(s1, s2):\n",
    "    # Copied from (1st version): https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshteinDist(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "def levenshteinSim(s1, s2_sr):\n",
    "    # # # # # # # # # #\n",
    "    # Compute the similarity between a string and a vector of strings based on the levenshtein distance\n",
    "    # similarity = 1 - (levenshtein(s1,s2)/max(len(s1),len(s2)))\n",
    "    # Args:\n",
    "    #       s1 - (string)\n",
    "    #       s2_sr - (numpy series of strings)\n",
    "    # Return:\n",
    "    #       sim_list - (list of floats), list of similarities\n",
    "    # # # # # # # # # #\n",
    "    sim_list = []\n",
    "    for s2 in s2_sr:\n",
    "        if(len(s1) == 0 and len(s2) == 0): # both are empty\n",
    "            sim_list.append(1)\n",
    "        else:\n",
    "            sim_list.append(1 - (levenshteinDist(s1,s2)/max(len(s1), len(s2))))\n",
    "    return sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # PREDICTION # # # # # # \n",
    "\n",
    "def predictMatch(p1_df, p2_df, total_sim_threshold=2.5, tf_threshold=0.012):\n",
    "    # # # # # # # # # #\n",
    "    # Predict equal items in 2 dataframes \n",
    "    # Args:\n",
    "    #       p1_df - (pandas dataframe)\n",
    "    #       p2_df - (pandas dataframe)\n",
    "    #       total_sim_threshold - (float [0,4]), represents the threshold for defining a match\n",
    "    #       tf_threshold - (float [0,1]), represents the threshold for defining the stop words    \n",
    "    # Return:\n",
    "    #       matched_keys_list - (list of tuples of strings), list of the matched keys (p1.key, p2.key)\n",
    "    # # # # # # # # # #\n",
    "    \n",
    "    # hote_name tfidf matrices:\n",
    "    print(\"\\nApply tf-idf on hotel_name\")\n",
    "    p1_hotel_tfidf_csr, p2_hotel_tfidf_csr = getTfidfFeatures(p1_df['p1.hotel_name'], \n",
    "                                                                  p2_df['p2.hotel_name'], \n",
    "                                                                  tf_threshold=tf_threshold)    \n",
    "    \"\"\"\n",
    "        # hotel_address tfidf matrices:\n",
    "        print(\"\\nApply tf-idf on hotel_address\")\n",
    "        print(type(p1_df['p1.hotel_address'].values.astype('U')))\n",
    "        print(p1_df['p1.hotel_address'].values.astype('U'))\n",
    "\n",
    "        #p1_add_tfidf_csr, p2_add_tfidf_csr = getTfidfFeatures(p1_df['p1.hotel_address'].values.astype('U'), \n",
    "        #                                                          p2_df['p2.hotel_address'].values.astype('U'), \n",
    "        #                                                          tf_threshold=0.012)    \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    matched_keys_list = []\n",
    "    debug_matched_df = pd.DataFrame(columns=(list(p1_df.columns) + list(p2_df.columns)))\n",
    "\n",
    "    for country in set(p1_df['p1.country_code']):\n",
    "        p1_country_idx = getDfSliceRowIdx(p1_df, colName='p1.country_code', val=country)\n",
    "        p2_country_idx = getDfSliceRowIdx(p2_df, colName='p2.country_code', val=country)\n",
    "        print((\"\\nDEBUG: country = %s; len(p1_country_idx): %d; len(p2_country_idx): %d\") % \n",
    "              (country, len(p1_country_idx), len(p2_country_idx)))\n",
    "\n",
    "        p2_not_matched_idx_list = list(p2_country_idx)\n",
    "        \n",
    "        for i in p1_country_idx:\n",
    "            # If there are still p2 items left (not matched with the previous p1 items of this country)\n",
    "            if(len(p2_not_matched_idx_list) > 0): \n",
    "                # p2_cols_sim_df is a dataframe that will contain the similarity scores of each column value of p1 row (index=i)\n",
    "                # with all related p2 rows (same country):\n",
    "                p2_cols_sim_df = pd.DataFrame()\n",
    "                p2_cols_sim_df['p2_idx'] = p2_country_idx\n",
    "\n",
    "                # hotel_name similarity\n",
    "                p2_cols_sim_df['hotel_name'] = calcCosineSim(p1_hotel_tfidf_csr[i,:], \n",
    "                                                             p2_hotel_tfidf_csr[p2_country_idx,:])\n",
    "                # city_name similarity:\n",
    "                p2_cols_sim_df['city_name'] = levenshteinSim(p1_df.loc[i,'p1.city_name'], \n",
    "                                                             p2_df.loc[p2_country_idx,'p2.city_name'])  \n",
    "                # postal_code similarity:\n",
    "                p2_cols_sim_df['postal_code'] = levenshteinSim(str(p1_df.loc[i,'p1.postal_code']), \n",
    "                                                               p2_df.loc[p2_country_idx,'p2.postal_code'].apply(lambda x: str(x)))\n",
    "                # star_rating similarity:\n",
    "                p2_cols_sim_df['star_rating'] = list(p2_df.loc[p2_country_idx,'p2.star_rating'].apply(\n",
    "                                                lambda x: (1 - abs(x - p1_df.loc[i,'p1.star_rating']))))\n",
    "\n",
    "\n",
    "                # Rename p2_cols_sim_df indices (original p2_df indices)\n",
    "                p2_cols_sim_df = p2_cols_sim_df.set_index('p2_idx')\n",
    "\n",
    "                #print((\"\\n\\nDEBUG: i = %d;\\np2_cols_sim_df:\\n%s\") %(i, p2_cols_sim_df))\n",
    "\n",
    "                # Sum all similarity scores - represents the total similarity of p1 row (index=i) \n",
    "                # with all related p2 rows (same country):\n",
    "                p2_total_sims = p2_cols_sim_df.sum(axis=1)\n",
    "\n",
    "                #print(p2_total_sims[p2_not_matched_idx_list])\n",
    "\n",
    "                # Pick the most similar available p2 row \n",
    "                # (available = not matched yet with any of the previouse p1 rows):\n",
    "                p2_max_sim_idx = p2_total_sims[p2_not_matched_idx_list].idxmax()\n",
    "                #print((\"\\nDEBUG: p2_max_sim_idx: %s\") % str(p2_max_sim_idx))\n",
    "                if p2_total_sims[p2_max_sim_idx] >= total_sim_threshold:\n",
    "                    matched_keys_list.append((p1_df.loc[i,'p1.key'], p2_df.loc[p2_max_sim_idx, 'p2.key']))\n",
    "                    debug_matched_df.loc[debug_matched_df.shape[0]] = list(p1_df.iloc[i]) + list(p2_df.iloc[p2_max_sim_idx])\n",
    "                    #print((\"\\nDEBUG: p2_not_matched_idx_list BEFORE remove: %s\") % p2_not_matched_idx_list)\n",
    "                    p2_not_matched_idx_list.remove(p2_max_sim_idx)\n",
    "                    #print((\"\\nDEBUG: p2_not_matched_idx_list AFTER remove: %s\") % p2_not_matched_idx_list)\n",
    "\n",
    "    #print((\"\\nMatched keys: %s\") % matched_keys_list)\n",
    "\n",
    "    # TODO: don't return debug_matched_df\n",
    "    return matched_keys_list, debug_matched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # EVALUATION # # # # # # \n",
    "\n",
    "def estimateResultsPerformance(pred_match_list, known_match_list):\n",
    "    # # # # # # # # # #\n",
    "    # Estimate the precision and recall of predictions compared to known matches\n",
    "    # Args:\n",
    "    #       pred_match_list - (list of tuples), (p1.key, p2.key) list of predicted matches\n",
    "    #       known_match_list - (list of tuples), (p1.key, p2.key) list of ground truth\n",
    "    # Return:\n",
    "    #       Nothing\n",
    "    # # # # # # # # # #\n",
    "    \n",
    "    known_p1, known_p2 = zip(*known_match_list) # unzip known_match_list to p1.key and p2.key lists\n",
    "    # Extract the relevant predictions for the evaluation procedure (p1.key or p2.key exist in known_match_list)\n",
    "    pred_to_eval = [t for t in pred_match_list if (t[0] in known_p1 or t[1] in known_p2)]\n",
    "    valid_pred = set(pred_to_eval) & set(known_match_list)\n",
    "    # Precision (accuracy) estimation:\n",
    "    print((\"\\nEstimated precision: %f\") % (len(valid_pred)/len(pred_to_eval)))\n",
    "    # Recall (coverage) estimation:\n",
    "    print((\"\\nEstimated recall: %f\") % (len(valid_pred)/len(known_match_list)))\n",
    "    print((\"\\nDEBUG: len(pred_match_list): %d; len(known_match_list): %d; \\nlen(pred_to_eval): %d; lev(valid_pred): %d;\") %\n",
    "         (len(pred_match_list), len(known_match_list), len(pred_to_eval), len(valid_pred)))\n",
    "    \n",
    "    return\n",
    "\n",
    "def evaluate(p1_df, p2_df, example_df, total_sim_threshold=3., tf_threshold=0.1):\n",
    "    # # # # # # # # # #\n",
    "    # Evaluate the prediction results when trained 2 dataframes, where example_df is a subset/all of the true matches\n",
    "    # Args:\n",
    "    #       p1_df - (pandas datafame)\n",
    "    #       p2_df - (pandas datafame)\n",
    "    #       example_df - (pandas datafame)\n",
    "    #       total_sim_threshold - (float [0,4]), represents the threshold for defining a match\n",
    "    #       tf_threshold - (float [0,1]), represents the threshold for defining the stop words    \n",
    "    # Return:\n",
    "    #       Nothing\n",
    "    # # # # # # # # # #\n",
    "    \n",
    "    # Preprocess:\n",
    "    p1_df = preprocData(p1_df)\n",
    "    p2_df = preprocData(p2_df)\n",
    "    \n",
    "    # Predict matches:\n",
    "    pred_matched_keys_list, debug_matched_df = predictMatch(p1_df, \n",
    "                                                            p2_df, \n",
    "                                                            total_sim_threshold,\n",
    "                                                            tf_threshold)\n",
    "    \n",
    "    print((\"DEBUG: len(pred_matched_keys_list): %d\\ndebug_matched_df.shape: %s\") % (len(pred_matched_keys_list), debug_matched_df.shape))\n",
    "    \n",
    "    # Extract the matched keys from example_df:\n",
    "    matched_keys_list = list(zip(list(example_df.loc[:,'p1.key']), list(example_df.loc[:,'p2.key'])))\n",
    "    \n",
    "    # Evaluate the prediction results:\n",
    "    estimateResultsPerformance(pred_matched_keys_list, matched_keys_list)\n",
    "    \n",
    "    return\n",
    "\n",
    "def evaluateOnExampleOnly(example_df, total_sim_threshold=3., tf_threshold=0.1):\n",
    "    # # # # # # # # # #\n",
    "    # Evaluate the prediction results when trained on the example file only\n",
    "    # Args:\n",
    "    #       example_df - (pandas datafame)\n",
    "    #       total_sim_threshold - (float [0,4]), represents the threshold for defining a match\n",
    "    #       tf_threshold - (float [0,1]), represents the threshold for defining the stop words    \n",
    "    # Return:\n",
    "    #       Nothing\n",
    "    # # # # # # # # # #\n",
    "    \n",
    "    # Split example_df to 2 separated dataframes:\n",
    "    p1_cols = [x.startswith(\"p1.\") for x in example_df.columns]\n",
    "    p2_cols = [x.startswith(\"p2.\") for x in example_df.columns]\n",
    "    p1_df = match_example_df.loc[:, p1_cols]\n",
    "    p2_df = match_example_df.loc[:, p2_cols]\n",
    "    print((\"DEBUG: p1_df.shape: %s\\p2_df.shape: %s\") % (p1_df.shape, p2_df.shape))\n",
    "    \n",
    "    # Evaluate:\n",
    "    evaluate(p1_df, p2_df, example_df, total_sim_threshold, tf_threshold)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "xls = pd.ExcelFile(input_file_path)\n",
    "\n",
    "#p1_df = xls.parse(p1_sheet_name, encoding='utf-8')\n",
    "#p2_df = xls.parse(p2_sheet_name, encoding='utf-8')\n",
    "example_df = xls.parse(example_sheet_name, encoding='utf-8')\n",
    "evaluateOnExampleOnly(example_df, total_sim_threshold=2., tf_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataAnalysis(match_example_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apply tf-idf on hotel_name\n",
      "\n",
      "Vocabulary length: 10843\n",
      "\n",
      "Data-driven stop words - appeared in more than 240 of the transactions:\n",
      "{'house', 'hotel', 'the', 'boutique', 'beach', 'spa', 'grand', 'city', 'guesthouse', 'park', 'quality', 'guest', 'hostel', 'branch', 'inn', 'comfort', 'suites', 'resort', 'road', 'apartment', 'villa', 'and', 'lodge', 'airport', 'by', 'motel', 'best', 'western', 'apartments'}\n",
      "\n",
      "DEBUG: country = ; len(p1_country_idx): 5; len(p2_country_idx): 5\n",
      "\n",
      "DEBUG: country = GR; len(p1_country_idx): 76; len(p2_country_idx): 76\n",
      "\n",
      "DEBUG: country = AR; len(p1_country_idx): 17; len(p2_country_idx): 17\n",
      "\n",
      "DEBUG: country = GH; len(p1_country_idx): 2; len(p2_country_idx): 2\n",
      "\n",
      "DEBUG: country = DE; len(p1_country_idx): 152; len(p2_country_idx): 152\n",
      "\n",
      "DEBUG: country = LV; len(p1_country_idx): 3; len(p2_country_idx): 3\n",
      "\n",
      "DEBUG: country = FI; len(p1_country_idx): 2; len(p2_country_idx): 2\n",
      "\n",
      "DEBUG: country = MM; len(p1_country_idx): 65; len(p2_country_idx): 65\n",
      "\n",
      "DEBUG: country = HR; len(p1_country_idx): 10; len(p2_country_idx): 10\n",
      "\n",
      "DEBUG: country = CR; len(p1_country_idx): 4; len(p2_country_idx): 4\n",
      "\n",
      "DEBUG: country = XK; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = HK; len(p1_country_idx): 41; len(p2_country_idx): 41\n",
      "\n",
      "DEBUG: country = FR; len(p1_country_idx): 267; len(p2_country_idx): 267\n",
      "\n",
      "DEBUG: country = TW; len(p1_country_idx): 231; len(p2_country_idx): 231\n",
      "\n",
      "DEBUG: country = EE; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = CY; len(p1_country_idx): 4; len(p2_country_idx): 4\n",
      "\n",
      "DEBUG: country = JM; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = HU; len(p1_country_idx): 55; len(p2_country_idx): 55\n",
      "\n",
      "DEBUG: country = KW; len(p1_country_idx): 9; len(p2_country_idx): 9\n",
      "\n",
      "DEBUG: country = FJ; len(p1_country_idx): 12; len(p2_country_idx): 12\n",
      "\n",
      "DEBUG: country = US; len(p1_country_idx): 962; len(p2_country_idx): 962\n",
      "\n",
      "DEBUG: country = AU; len(p1_country_idx): 539; len(p2_country_idx): 539\n",
      "\n",
      "DEBUG: country = NO; len(p1_country_idx): 7; len(p2_country_idx): 7\n",
      "\n",
      "DEBUG: country = TN; len(p1_country_idx): 6; len(p2_country_idx): 6\n",
      "\n",
      "DEBUG: country = MZ; len(p1_country_idx): 2; len(p2_country_idx): 2\n",
      "\n",
      "DEBUG: country = PF; len(p1_country_idx): 10; len(p2_country_idx): 10\n",
      "\n",
      "DEBUG: country = MX; len(p1_country_idx): 22; len(p2_country_idx): 22\n",
      "\n",
      "DEBUG: country = DK; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = KZ; len(p1_country_idx): 3; len(p2_country_idx): 3\n",
      "\n",
      "DEBUG: country = NG; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = BH; len(p1_country_idx): 14; len(p2_country_idx): 14\n",
      "\n",
      "DEBUG: country = PT; len(p1_country_idx): 24; len(p2_country_idx): 24\n",
      "\n",
      "DEBUG: country = TO; len(p1_country_idx): 2; len(p2_country_idx): 2\n",
      "\n",
      "DEBUG: country = GB; len(p1_country_idx): 360; len(p2_country_idx): 360\n",
      "\n",
      "DEBUG: country = DO; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = TZ; len(p1_country_idx): 12; len(p2_country_idx): 12\n",
      "\n",
      "DEBUG: country = LU; len(p1_country_idx): 2; len(p2_country_idx): 2\n",
      "\n",
      "DEBUG: country = ES; len(p1_country_idx): 251; len(p2_country_idx): 251\n",
      "\n",
      "DEBUG: country = EG; len(p1_country_idx): 13; len(p2_country_idx): 13\n",
      "\n",
      "DEBUG: country = RO; len(p1_country_idx): 10; len(p2_country_idx): 10\n",
      "\n",
      "DEBUG: country = OM; len(p1_country_idx): 15; len(p2_country_idx): 15\n",
      "\n",
      "DEBUG: country = LK; len(p1_country_idx): 214; len(p2_country_idx): 214\n",
      "\n",
      "DEBUG: country = PK; len(p1_country_idx): 2; len(p2_country_idx): 2\n",
      "\n",
      "DEBUG: country = QA; len(p1_country_idx): 9; len(p2_country_idx): 9\n",
      "\n",
      "DEBUG: country = MN; len(p1_country_idx): 2; len(p2_country_idx): 2\n",
      "\n",
      "DEBUG: country = BD; len(p1_country_idx): 9; len(p2_country_idx): 9\n",
      "\n",
      "DEBUG: country = UG; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = BO; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = TH; len(p1_country_idx): 948; len(p2_country_idx): 948\n",
      "\n",
      "DEBUG: country = CA; len(p1_country_idx): 47; len(p2_country_idx): 47\n",
      "\n",
      "DEBUG: country = BA; len(p1_country_idx): 3; len(p2_country_idx): 3\n",
      "\n",
      "DEBUG: country = ZA; len(p1_country_idx): 189; len(p2_country_idx): 189\n",
      "\n",
      "DEBUG: country = PR; len(p1_country_idx): 4; len(p2_country_idx): 4\n",
      "\n",
      "DEBUG: country = CZ; len(p1_country_idx): 36; len(p2_country_idx): 36\n",
      "\n",
      "DEBUG: country = NL; len(p1_country_idx): 35; len(p2_country_idx): 35\n",
      "\n",
      "DEBUG: country = PL; len(p1_country_idx): 10; len(p2_country_idx): 10\n",
      "\n",
      "DEBUG: country = IE; len(p1_country_idx): 11; len(p2_country_idx): 11\n",
      "\n",
      "DEBUG: country = NP; len(p1_country_idx): 38; len(p2_country_idx): 38\n",
      "\n",
      "DEBUG: country = SA; len(p1_country_idx): 66; len(p2_country_idx): 66\n",
      "\n",
      "DEBUG: country = CV; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = LT; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = SI; len(p1_country_idx): 3; len(p2_country_idx): 3\n",
      "\n",
      "DEBUG: country = MK; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = IT; len(p1_country_idx): 287; len(p2_country_idx): 287\n",
      "\n",
      "DEBUG: country = RU; len(p1_country_idx): 74; len(p2_country_idx): 74\n",
      "\n",
      "DEBUG: country = JO; len(p1_country_idx): 18; len(p2_country_idx): 18\n",
      "\n",
      "DEBUG: country = NZ; len(p1_country_idx): 185; len(p2_country_idx): 185\n",
      "\n",
      "DEBUG: country = BG; len(p1_country_idx): 5; len(p2_country_idx): 5\n",
      "\n",
      "DEBUG: country = CL; len(p1_country_idx): 3; len(p2_country_idx): 3\n",
      "\n",
      "DEBUG: country = RE; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = CH; len(p1_country_idx): 30; len(p2_country_idx): 30\n",
      "\n",
      "DEBUG: country = VN; len(p1_country_idx): 364; len(p2_country_idx): 364\n",
      "\n",
      "DEBUG: country = MG; len(p1_country_idx): 2; len(p2_country_idx): 2\n",
      "\n",
      "DEBUG: country = KH; len(p1_country_idx): 112; len(p2_country_idx): 112\n",
      "\n",
      "DEBUG: country = SG; len(p1_country_idx): 39; len(p2_country_idx): 39\n",
      "\n",
      "DEBUG: country = MU; len(p1_country_idx): 22; len(p2_country_idx): 22\n",
      "\n",
      "DEBUG: country = MP; len(p1_country_idx): 3; len(p2_country_idx): 3\n",
      "\n",
      "DEBUG: country = BN; len(p1_country_idx): 6; len(p2_country_idx): 6\n",
      "\n",
      "DEBUG: country = BT; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = SC; len(p1_country_idx): 17; len(p2_country_idx): 17\n",
      "\n",
      "DEBUG: country = MA; len(p1_country_idx): 33; len(p2_country_idx): 33\n",
      "\n",
      "DEBUG: country = AE; len(p1_country_idx): 68; len(p2_country_idx): 68\n",
      "\n",
      "DEBUG: country = AT; len(p1_country_idx): 35; len(p2_country_idx): 35\n",
      "\n",
      "DEBUG: country = BE; len(p1_country_idx): 10; len(p2_country_idx): 10\n",
      "\n",
      "DEBUG: country = VU; len(p1_country_idx): 4; len(p2_country_idx): 4\n",
      "\n",
      "DEBUG: country = ID; len(p1_country_idx): 599; len(p2_country_idx): 599\n",
      "\n",
      "DEBUG: country = BR; len(p1_country_idx): 6; len(p2_country_idx): 6\n",
      "\n",
      "DEBUG: country = PA; len(p1_country_idx): 5; len(p2_country_idx): 5\n",
      "\n",
      "DEBUG: country = WS; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = ET; len(p1_country_idx): 3; len(p2_country_idx): 3\n",
      "\n",
      "DEBUG: country = MO; len(p1_country_idx): 7; len(p2_country_idx): 7\n",
      "\n",
      "DEBUG: country = KE; len(p1_country_idx): 12; len(p2_country_idx): 12\n",
      "\n",
      "DEBUG: country = SK; len(p1_country_idx): 3; len(p2_country_idx): 3\n",
      "\n",
      "DEBUG: country = AZ; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = MY; len(p1_country_idx): 264; len(p2_country_idx): 264\n",
      "\n",
      "DEBUG: country = MT; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = LA; len(p1_country_idx): 56; len(p2_country_idx): 56\n",
      "\n",
      "DEBUG: country = IL; len(p1_country_idx): 49; len(p2_country_idx): 49\n",
      "\n",
      "DEBUG: country = RS; len(p1_country_idx): 3; len(p2_country_idx): 3\n",
      "\n",
      "DEBUG: country = IS; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = CK; len(p1_country_idx): 4; len(p2_country_idx): 4\n",
      "\n",
      "DEBUG: country = GU; len(p1_country_idx): 2; len(p2_country_idx): 2\n",
      "\n",
      "DEBUG: country = IN; len(p1_country_idx): 788; len(p2_country_idx): 788\n",
      "\n",
      "DEBUG: country = UA; len(p1_country_idx): 13; len(p2_country_idx): 13\n",
      "\n",
      "DEBUG: country = AL; len(p1_country_idx): 4; len(p2_country_idx): 4\n",
      "\n",
      "DEBUG: country = LB; len(p1_country_idx): 3; len(p2_country_idx): 3\n",
      "\n",
      "DEBUG: country = PH; len(p1_country_idx): 166; len(p2_country_idx): 166\n",
      "\n",
      "DEBUG: country = PG; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = JP; len(p1_country_idx): 557; len(p2_country_idx): 557\n",
      "\n",
      "DEBUG: country = CI; len(p1_country_idx): 2; len(p2_country_idx): 2\n",
      "\n",
      "DEBUG: country = TR; len(p1_country_idx): 121; len(p2_country_idx): 121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: country = CO; len(p1_country_idx): 13; len(p2_country_idx): 13\n",
      "\n",
      "DEBUG: country = PW; len(p1_country_idx): 3; len(p2_country_idx): 3\n",
      "\n",
      "DEBUG: country = SE; len(p1_country_idx): 4; len(p2_country_idx): 4\n",
      "\n",
      "DEBUG: country = MV; len(p1_country_idx): 27; len(p2_country_idx): 27\n",
      "\n",
      "DEBUG: country = UY; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = ZW; len(p1_country_idx): 2; len(p2_country_idx): 2\n",
      "\n",
      "DEBUG: country = BW; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = ME; len(p1_country_idx): 2; len(p2_country_idx): 2\n",
      "\n",
      "DEBUG: country = PE; len(p1_country_idx): 3; len(p2_country_idx): 3\n",
      "\n",
      "DEBUG: country = NF; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = CN; len(p1_country_idx): 878; len(p2_country_idx): 878\n",
      "\n",
      "DEBUG: country = KR; len(p1_country_idx): 226; len(p2_country_idx): 226\n",
      "\n",
      "DEBUG: country = NC; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "\n",
      "DEBUG: country = MD; len(p1_country_idx): 1; len(p2_country_idx): 1\n",
      "DEBUG: len(pred_matched_keys_list): 8777\n",
      "debug_matched_df.shape: (8777, 14)\n",
      "\n",
      "Estimated precision: 0.000000\n",
      "\n",
      "Estimated recall: 0.000000\n",
      "\n",
      "DEBUG: len(pred_match_list): 8777; len(known_match_list): 499; \n",
      "len(pred_to_eval): 1; lev(valid_pred): 0;\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "xls = pd.ExcelFile(input_file_path)\n",
    "\n",
    "p1_df = xls.parse(p1_sheet_name, encoding='utf-8')\n",
    "p2_df = xls.parse(p2_sheet_name, encoding='utf-8')\n",
    "example_df = xls.parse(example_sheet_name, encoding='utf-8')\n",
    "evaluate(p1_df, p2_df, example_df, total_sim_threshold=2.5, tf_threshold=0.012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_example_df = preprocData(match_example_df)\n",
    "p1_cols = [x.startswith(\"p1.\") for x in match_example_df.columns]\n",
    "p2_cols = [x.startswith(\"p2.\") for x in match_example_df.columns]\n",
    "print((\"DEBUG: p1_cols: %s\\np2_cols: %s\") % (p1_cols, p2_cols))\n",
    "p1_df = match_example_df.loc[:, p1_cols]\n",
    "p2_df = match_example_df.loc[:, p2_cols]\n",
    "print((\"DEBUG: p1_df.shape: %s\\p2_df.shape: %s\") % (p1_df.shape, p2_df.shape))\n",
    "\n",
    "matched_keys_list, debug_matched_df = predictMatch(p1_df, \n",
    "                                                   p2_df, \n",
    "                                                   total_sim_threshold=2.,\n",
    "                                                   tf_threshold=0.1)\n",
    "print((\"DEBUG: len(matched_keys_list): %d\\ndebug_matched_df.shape: %s\") % (len(matched_keys_list), debug_matched_df.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_keys = list(zip(list(match_example_df.loc[:,'p1.key']), list(match_example_df.loc[:,'p2.key'])))\n",
    "\n",
    "estimateResultsPerformance(predMatchList=matched_keys_list, knownMatchList=example_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_matched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(['p1.postal_code', 'p2.postal_code']) & set(df.columns)\n",
    "a = [('a','h'), ('c','b')]\n",
    "b = [('a','k'), ('p','h'), ('c','b'), ('d','d')]\n",
    "a1, a2 = zip(*a)\n",
    "b1, b2 = zip(*b)\n",
    "relevant_key1 = np.array([i in a1 for i in b1])\n",
    "relevant_key2 = np.array([i in a2 for i in b2])\n",
    "relevant_elements = np.where(relevant_key1 | relevant_key2)[0]\n",
    "print(relevant_elements)\n",
    "print(type(relevant_elements))\n",
    "new_b1 = [i for i in b if (i[0] in a1 or i[1] in a2)]\n",
    "new_b1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len(p1_in_known): 1; len(p2_in_known): 0\n"
     ]
    }
   ],
   "source": [
    "matched_keys_list = list(zip(list(example_df.loc[:,'p1.key']), list(example_df.loc[:,'p2.key'])))\n",
    "known_p1, known_p2 = zip(*matched_keys_list) # unzip known_match_list to p1.key and p2.key lists\n",
    "p1_in_known = set(list(p1_df.loc[:,'p1.key'])) & set(known_p1)\n",
    "p2_in_known = set(list(p2_df.loc[:,'p2.key'])) & set(known_p2)\n",
    "print((\"\\nlen(p1_in_known): %d; len(p2_in_known): %d\") % (len(p1_in_known), len(p2_in_known)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
